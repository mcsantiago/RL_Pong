{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Import dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"Using TensorFlow backend.\n"}],"source":["import random\n","import gym\n","import numpy as np\n","import os\n","from collections import deque\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, Flatten\n","from keras.optimizers import Adam"]},{"cell_type":"markdown","metadata":{},"source":["### Set parameters"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["env = gym.make('Pong-v0')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":"Box(210, 160, 3)"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["state_size = env.observation_space\n","state_size"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":"6"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["action_size = env.action_space.n\n","action_size"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["batch_size = 32\n","n_episodes = 10000000"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["output_dir = 'model_output/cartpols'\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)"]},{"cell_type":"markdown","metadata":{},"source":["### Define agent"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class DQNAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        \n","        self.memory = deque(maxlen=30000)\n","        \n","        self.gamma = 0.95\n","        \n","        self.epsilon = 1.0\n","        self.epsilon_decay = 0.0000009\n","        self.epsilon_min = 0.1\n","        \n","        self.learning_rate = 0.001\n","        \n","        self.model = self._build_model()\n","        \n","    def _build_model(self):\n","        model = Sequential()\n","    \n","        model.add(Conv2D(1, kernel_size=3, activation='relu', input_shape=(105, 80, 1)))\n","        model.add(Conv2D(16, kernel_size=8, strides=4, activation='relu'))\n","        model.add(Conv2D(32, kernel_size=4, strides=2, activation='relu'))\n","        model.add(Flatten())\n","        model.add(Dense(256, activation='relu'))\n","        model.add(Dense(self.action_size, activation='linear'))\n","        \n","        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n","        \n","        return model\n","    \n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon -= self.epsilon_decay\n","        \n","    def act(self, state):\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        state = np.expand_dims(state, axis=0)\n","        state = np.expand_dims(state, axis=3)\n","        act_values = self.model.predict(state)\n","        return np.argmax(act_values[0])\n","    \n","    def rgb2gray(self, rgb):\n","        return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n","    \n","    def replay(self, batch_size):\n","        minibatch = random.sample(self.memory, batch_size)\n","        \n","        for state, action, reward, next_state, done in minibatch:\n","            target = reward\n","            if not done:\n","                next_state = np.expand_dims(next_state, axis=0)\n","                next_state = np.expand_dims(next_state, axis=3)\n","                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n","            state = np.expand_dims(state, axis=0)\n","            state = np.expand_dims(state, axis=3)\n","            target_f = self.model.predict(state)\n","            target_f[0][action] = target\n","            \n","            self.model.fit(state, target_f, epochs=1, verbose=0)\n","            \n","\n","            \n","    \n","    def load(self, name):\n","        self.model.load_weights(name)\n","        \n","    def save(self, name):\n","        self.model.save_weights(name)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["agent = DQNAgent(state_size, action_size)"]},{"cell_type":"markdown","metadata":{},"source":["### Preprocessing functions"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def rgb2gray(rgb):\n","    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])"]},{"cell_type":"markdown","metadata":{},"source":["### Interact with environment"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":"(105, 80)"},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["state = env.reset()\n","state = rgb2gray(state)\n","state = state[::2,::2]\n","state.shape"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"frame: 1224/10000000, score: -1.0, e: 1.0\nframe: 2434/10000000, score: -1.0, e: 1.0\nframe: 3919/10000000, score: -1.0, e: 1.0\nframe: 5134/10000000, score: -1.0, e: 1.0\nframe: 6327/10000000, score: -1.0, e: 0.99\nframe: 7916/10000000, score: -1.0, e: 0.99\nframe: 9009/10000000, score: -1.0, e: 0.99\nframe: 10425/10000000, score: -1.0, e: 0.99\nframe: 11958/10000000, score: -1.0, e: 0.99\nframe: 13157/10000000, score: -1.0, e: 0.99\nframe: 14365/10000000, score: -1.0, e: 0.99\nframe: 15673/10000000, score: -1.0, e: 0.99\nframe: 17052/10000000, score: -1.0, e: 0.98\nframe: 18298/10000000, score: -1.0, e: 0.98\nframe: 19705/10000000, score: -1.0, e: 0.98\nframe: 21152/10000000, score: -1.0, e: 0.98\nframe: 22345/10000000, score: -1.0, e: 0.98\nframe: 23463/10000000, score: -1.0, e: 0.98\nframe: 24789/10000000, score: -1.0, e: 0.98\nframe: 26055/10000000, score: -1.0, e: 0.98\nframe: 27066/10000000, score: -1.0, e: 0.98\nframe: 28156/10000000, score: -1.0, e: 0.97\nframe: 29641/10000000, score: -1.0, e: 0.97\nframe: 30657/10000000, score: -1.0, e: 0.97\nframe: 31769/10000000, score: -1.0, e: 0.97\nframe: 32780/10000000, score: -1.0, e: 0.97\nframe: 34017/10000000, score: -1.0, e: 0.97\nframe: 35336/10000000, score: -1.0, e: 0.97\nframe: 36465/10000000, score: -1.0, e: 0.97\nframe: 37586/10000000, score: -1.0, e: 0.97\nframe: 38985/10000000, score: -1.0, e: 0.96\nframe: 40294/10000000, score: -1.0, e: 0.96\nframe: 41427/10000000, score: -1.0, e: 0.96\nframe: 42801/10000000, score: -1.0, e: 0.96\nframe: 44250/10000000, score: -1.0, e: 0.96\nframe: 45522/10000000, score: -1.0, e: 0.96\nframe: 46626/10000000, score: -1.0, e: 0.96\nframe: 47818/10000000, score: -1.0, e: 0.96\nframe: 48958/10000000, score: -1.0, e: 0.96\nframe: 50014/10000000, score: -1.0, e: 0.95\nframe: 51446/10000000, score: -1.0, e: 0.95\nframe: 52732/10000000, score: -1.0, e: 0.95\nframe: 54065/10000000, score: -1.0, e: 0.95\nframe: 55299/10000000, score: -1.0, e: 0.95\nframe: 56536/10000000, score: -1.0, e: 0.95\nframe: 57909/10000000, score: -1.0, e: 0.95\nframe: 59309/10000000, score: -1.0, e: 0.95\nframe: 60645/10000000, score: -1.0, e: 0.95\nframe: 61668/10000000, score: -1.0, e: 0.94\nframe: 62835/10000000, score: -1.0, e: 0.94\nframe: 64031/10000000, score: -1.0, e: 0.94\nframe: 65159/10000000, score: -1.0, e: 0.94\nframe: 66259/10000000, score: -1.0, e: 0.94\nframe: 67518/10000000, score: -1.0, e: 0.94\nframe: 68698/10000000, score: -1.0, e: 0.94\nframe: 69757/10000000, score: -1.0, e: 0.94\nframe: 71249/10000000, score: -1.0, e: 0.94\nframe: 72465/10000000, score: -1.0, e: 0.93\nframe: 73658/10000000, score: -1.0, e: 0.93\nframe: 75075/10000000, score: -1.0, e: 0.93\nframe: 76269/10000000, score: -1.0, e: 0.93\nframe: 77497/10000000, score: -1.0, e: 0.93\nframe: 78695/10000000, score: -1.0, e: 0.93\nframe: 79819/10000000, score: -1.0, e: 0.93\n"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-96390c3b23d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb2gray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/repos/venvs/sandbox/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/repos/venvs/sandbox/lib/python3.7/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"ale.lives\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/repos/venvs/sandbox/lib/python3.7/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'image'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/repos/venvs/sandbox/lib/python3.7/site-packages/gym/envs/atari/atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/repos/venvs/sandbox/lib/python3.7/site-packages/atari_py/ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[0;34m(self, screen_data)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m480\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m         \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/repos/venvs/sandbox/lib/python3.7/site-packages/numpy/ctypeslib.py\u001b[0m in \u001b[0;36mas_ctypes\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    527\u001b[0m         \"\"\"Create and return a ctypes object from a numpy array.  Actually\n\u001b[1;32m    528\u001b[0m         anything that exposes the __array_interface__ is accepted.\"\"\"\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mai\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mai\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"strided arrays not supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["done = False\n","time = 0\n","episode = 0\n","while time < n_episodes:\n","    state = env.reset()\n","    state = rgb2gray(state)\n","    state = state[::2,::2]\n","    \n","    while not done:\n","        env.render()\n","        action = agent.act(state)\n","        next_state, reward, done, _ = env.step(action)\n","        next_state = rgb2gray(next_state)\n","        next_state = next_state[::2,::2]\n","        #next_state = next_state.reshape(-1, next_state.shape[0])\n","        reward = reward\n","        agent.remember(state, action, reward, next_state, done)\n","        state = next_state\n","        time += 1\n","    episode += 1\n","    print(\"frame: {}/{}, score: {}, e: {:.2}\" # print the episode's score and agent's epsilon\n","      .format(time, n_episodes, reward, agent.epsilon))\n","    done = False\n","    if len(agent.memory) > batch_size:\n","        agent.replay(batch_size) # train the agent by replaying the experiences of the episode\n","    if episode % 50 == 0: # save weights every 50th episode (game)\n","        agent.save(output_dir + \"weights_\" + '{:04d}'.format(time) + \".hdf5\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}