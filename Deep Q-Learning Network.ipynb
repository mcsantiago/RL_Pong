{"cells":[{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Import dependencies"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["import random\n","import gym\n","import numpy as np\n","import os\n","from collections import deque\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, Flatten\n","from keras.optimizers import Adam"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Set parameters"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["env = gym.make('Pong-v0')"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["state_size = env.observation_space\n","state_size"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["action_size = env.action_space.n\n","action_size"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["batch_size = 32\n","n_episodes = 10000000"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["output_dir = 'model_output/pong'\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Preprocessing functions"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["def rgb2gray(rgb):\n","    small_frame = np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n","    return small_frame"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["def preprocess_frame(frame):\n","    state = rgb2gray(frame)\n","    state = state[::2,::2]\n","    state = np.expand_dims(state, axis=0)\n","    state = np.expand_dims(state, axis=3)\n","    state = state[0:1, 25:110, 0:80, 0:1]\n","    return state"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Define agent"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["class DQNAgent:\n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        \n","        self.memory = deque(maxlen=40000)\n","        \n","        self.gamma = 0.95\n","        \n","        self.epsilon = 1.0\n","        self.epsilon_decay = 0.0000009\n","        self.epsilon_min = 0.1\n","        \n","        self.learning_rate = 0.001\n","        \n","        self.model = self._build_model()\n","        \n","    def _build_model(self):\n","        model = Sequential()\n","    \n","        model.add(Conv2D(1, kernel_size=3, activation='relu', input_shape=(80, 80, 1)))\n","        model.add(Conv2D(16, kernel_size=8, strides=4, activation='relu'))\n","        model.add(Conv2D(32, kernel_size=4, strides=2, activation='relu'))\n","        model.add(Flatten())\n","        model.add(Dense(256, activation='relu'))\n","        model.add(Dense(self.action_size, activation='linear'))\n","        \n","        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n","        \n","        return model\n","    \n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon -= self.epsilon_decay\n","        \n","    def act(self, state):\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        act_values = self.model.predict(state)\n","        return np.argmax(act_values[0])\n","    \n","    def replay(self, batch_size):\n","        minibatch = random.sample(self.memory, batch_size)\n","        \n","        for state, action, reward, next_state, done in minibatch:\n","            target = reward\n","            if not done:\n","                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n","            target_f = self.model.predict(state)\n","            target_f[0][action] = target\n","            \n","            self.model.fit(state, target_f, epochs=1, verbose=0)\n","    \n","    def load(self, name):\n","        self.model.load_weights(name)\n","        \n","    def save(self, name):\n","        self.model.save_weights(name)\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["agent = DQNAgent(state_size, action_size)"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["### Interact with environment"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["done = False\n","time = 0\n","episode = 0\n","max_score = 0\n","k = 4\n","while time < n_episodes:\n","    state = preprocess_frame(env.reset())\n","\n","    player_score = 0\n","    enemy_score = 0\n","    \n","    while not done:\n","        #env.render()\n","        action = agent.act(state)\n","        next_state, reward, done, _ = env.step(action)\n","\n","        if reward > 0: \n","            player_score += reward\n","        else: \n","            enemy_score -= reward\n","\n","        if time % k == 0: # only process every kth frame\n","            next_state = preprocess_frame(next_state)\n","            agent.remember(state, action, reward, next_state, done)\n","            state = next_state\n","        time += 1\n","    episode += 1\n","\n","    if player_score > max_score: \n","        max_score = player_score\n","    \n","    print(\"frame: {}/{}    enemy_score: {}    player_score: {}    max_score: {}    e: {:.2}\" # print the episode's score and agent's epsilon\n","      .format(time, n_episodes, enemy_score, player_score, max_score, agent.epsilon))\n","    \n","    done = False\n","    \n","    if len(agent.memory) > batch_size:\n","        agent.replay(batch_size) # train the agent by replaying the experiences of the episode\n","    \n","    if episode % 50 == 0: # save weights every 50th episode (game)\n","        agent.save(output_dir + \"weights_\" + '{:04d}'.format(episode) + \".hdf5\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}